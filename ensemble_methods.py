"""
–ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏:
1. –ë–ª–µ–Ω–¥–∏–Ω–≥ (–ø—Ä–æ—Å—Ç–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)
2. –°—Ç–µ–∫–∏–Ω–≥ (–º–µ—Ç–∞-–º–æ–¥–µ–ª—å)
3. –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤)
4. Hill Climbing (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤)

–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω—Å–∞–º–±–ª—è
"""

import pandas as pd
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from tqdm import tqdm
import time
import config
from data_loader import load_and_preprocess
from scipy.optimize import minimize

def load_predictions(include_nn=False, include_multi_seed_cat=True):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç OOF –∏ test predictions –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    print("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π...")
    
    models = ['lgbm', 'xgb', 'cat']
    if include_nn:
        models.append('nn')
    
    oof_preds = {}
    test_preds = {}
    
    for m in models:
        try:
            oof_file = f'oof_{m}.npy'
            test_file = f'test_{m}.npy'
            oof_preds[m] = np.load(oof_file)
            test_preds[m] = np.load(test_file)
            print(f"   ‚úÖ {m.upper()}: –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
        except FileNotFoundError:
            print(f"   ‚ö†Ô∏è  {m.upper()}: —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (oof_{m}.npy, test_{m}.npy)")
    
    # Load multi-seed CatBoost if available
    if include_multi_seed_cat:
        try:
            oof_multi = np.load('oof_cat_multi_seed.npy')
            test_multi = np.load('test_cat_multi_seed.npy')
            oof_preds['cat_multi_seed'] = oof_multi
            test_preds['cat_multi_seed'] = test_multi
            print(f"   ‚úÖ CAT_MULTI_SEED: –∑–∞–≥—Ä—É–∂–µ–Ω–æ (multi-seed ensemble)")
        except FileNotFoundError:
            print(f"   ‚ö†Ô∏è  CAT_MULTI_SEED: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–∑–∞–ø—É—Å—Ç–∏—Ç–µ train_cat_multi_seed.py)")
    
    if not oof_preds:
        raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏.")
    
    return oof_preds, test_preds

def simple_blending(oof_preds_dict, test_preds_dict):
    """–ü—Ä–æ—Å—Ç–æ–µ –±–ª–µ–Ω–¥–∏–Ω–≥ (—Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ)"""
    print("\nüìä –ú–µ—Ç–æ–¥ 1: Simple Blending (—Å—Ä–µ–¥–Ω–µ–µ –∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ)")
    
    oof_blend = np.mean(list(oof_preds_dict.values()), axis=0)
    test_blend = np.mean(list(test_preds_dict.values()), axis=0)
    
    return oof_blend, test_blend

def weighted_average(oof_preds_dict, test_preds_dict, y):
    """–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –≤–µ—Å–æ–≤"""
    print("\nüìä –ú–µ—Ç–æ–¥ 2: Weighted Average (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤)")
    
    def objective(weights):
        weights = weights / np.sum(weights)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        blended = np.zeros(len(y))
        for i, (model_name, preds) in enumerate(oof_preds_dict.items()):
            blended += weights[i] * preds
        return -roc_auc_score(y, blended)  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π AUC
    
    # –ù–∞—á–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ (—Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ)
    n_models = len(oof_preds_dict)
    initial_weights = np.ones(n_models) / n_models
    
    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: –≤–µ—Å–∞ >= 0, —Å—É–º–º–∞ = 1
    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
    bounds = [(0, 1) for _ in range(n_models)]
    
    print("   –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤...")
    result = minimize(objective, initial_weights, method='SLSQP', 
                     bounds=bounds, constraints=constraints, options={'maxiter': 100})
    
    optimal_weights = result.x / np.sum(result.x)
    
    print(f"   –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞:")
    for i, (model_name, _) in enumerate(oof_preds_dict.items()):
        print(f"      {model_name.upper()}: {optimal_weights[i]:.4f}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞
    oof_weighted = np.zeros(len(y))
    test_weighted = np.zeros(len(test_preds_dict[list(test_preds_dict.keys())[0]]))
    
    for i, (model_name, preds) in enumerate(oof_preds_dict.items()):
        oof_weighted += optimal_weights[i] * preds
        test_weighted += optimal_weights[i] * test_preds_dict[model_name]
    
    return oof_weighted, test_weighted, optimal_weights

def stacking(oof_preds_dict, test_preds_dict, y, meta_model_type='lgbm'):
    """
    –ß–µ—Å—Ç–Ω—ã–π —Å—Ç–µ–∫–∏–Ω–≥: –º–µ—Ç–∞-–º–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è out-of-fold.
    –ò–Ω–∞—á–µ (–µ—Å–ª–∏ –æ–±—É—á–∏—Ç—å –º–µ—Ç—É –Ω–∞ –≤—Å–µ—Ö OOF –∏ –æ—Ü–µ–Ω–∏—Ç—å –Ω–∞ –Ω–∏—Ö –∂–µ) CV –∑–∞–≤—ã—à–∞–µ—Ç—Å—è –∏ —á–∞—Å—Ç–æ –ø–∞–¥–∞–µ—Ç –Ω–∞ private.
    """
    meta_model_name = "LightGBM" if meta_model_type == 'lgbm' else "Logistic Regression"
    print(f"\nüìä –ú–µ—Ç–æ–¥ 3: Stacking (–º–µ—Ç–∞-–º–æ–¥–µ–ª—å: {meta_model_name}, OOF –¥–ª—è –º–µ—Ç—ã)")
    
    model_names = list(oof_preds_dict.keys())
    meta_features = np.column_stack([oof_preds_dict[m] for m in model_names])
    test_meta_features = np.column_stack([test_preds_dict[m] for m in model_names])
    
    skf = StratifiedKFold(n_splits=getattr(config, "N_SPLITS", 5), shuffle=True, random_state=1)
    meta_oof = np.zeros(len(y))
    meta_test = np.zeros(test_meta_features.shape[0])
    
    importance_sum = np.zeros(len(model_names), dtype=float)
    
    print("   –û–±—É—á–µ–Ω–∏–µ –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ –ø–æ —Ñ–æ–ª–¥–∞–º...")
    for tr_idx, va_idx in skf.split(meta_features, y):
        X_tr, X_va = meta_features[tr_idx], meta_features[va_idx]
        y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]
        
        if meta_model_type == 'lgbm':
            meta_model = lgb.LGBMClassifier(
                n_estimators=300,
                learning_rate=0.03,
                num_leaves=31,
                max_depth=5,
                reg_lambda=1.0,
                min_child_samples=50,
                subsample=0.8,
                colsample_bytree=0.8,
                metric='auc',
                verbosity=-1,
                random_state=1
            )
            meta_model.fit(X_tr, y_tr)
            importance_sum += meta_model.feature_importances_
        else:
            meta_model = LogisticRegression(max_iter=2000, random_state=1, C=0.5)
            meta_model.fit(X_tr, y_tr)
            importance_sum += meta_model.coef_[0]
        
        meta_oof[va_idx] = meta_model.predict_proba(X_va)[:, 1]
        meta_test += meta_model.predict_proba(test_meta_features)[:, 1] / skf.get_n_splits()
    
    print("   –í–∞–∂–Ω–æ—Å—Ç—å/–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –º–µ—Ç–∞-–º–æ–¥–µ–ª–∏ (—Å—É–º–º–∞ –ø–æ —Ñ–æ–ª–¥–∞–º):")
    for i, name in enumerate(model_names):
        print(f"      {name.upper():<20}: {importance_sum[i]:.2f}")
    
    return meta_oof, meta_test

def hill_climbing_blend(oof_preds_dict, test_preds_dict, y):
    """Hill Climbing –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–µ—Å–æ–≤"""
    print("\nüìä –ú–µ—Ç–æ–¥ 4: Hill Climbing Blend")
    
    n_models = len(oof_preds_dict)
    weights = np.ones(n_models) / n_models
    
    best_score = -roc_auc_score(y, np.mean(list(oof_preds_dict.values()), axis=0))
    best_weights = weights.copy()
    
    # Hill climbing
    step_size = 0.01
    max_iterations = 1000
    no_improvement = 0
    
    print("   –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (Hill Climbing)...")
    for iteration in tqdm(range(max_iterations), desc="   –ò—Ç–µ—Ä–∞—Ü–∏–∏", leave=False):
        improved = False
        
        for i in range(n_models):
            # –ü—Ä–æ–±—É–µ–º —É–≤–µ–ª–∏—á–∏—Ç—å –≤–µ—Å i-–π –º–æ–¥–µ–ª–∏
            new_weights = weights.copy()
            new_weights[i] += step_size
            
            # –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞
            other_weights_sum = np.sum(new_weights) - new_weights[i]
            if other_weights_sum > 0:
                for j in range(n_models):
                    if j != i:
                        new_weights[j] = new_weights[j] * (1 - new_weights[i]) / other_weights_sum
            
            new_weights = new_weights / np.sum(new_weights)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            
            # –í—ã—á–∏—Å–ª—è–µ–º score
            blended = np.zeros(len(y))
            for k, (_, preds) in enumerate(oof_preds_dict.items()):
                blended += new_weights[k] * preds
            
            score = -roc_auc_score(y, blended)
            
            if score < best_score:
                best_score = score
                best_weights = new_weights.copy()
                weights = new_weights.copy()
                improved = True
                no_improvement = 0
                break
        
        if not improved:
            no_improvement += 1
            if no_improvement >= 10:
                break
    
    print(f"   –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ (Hill Climbing):")
    for i, model_name in enumerate(oof_preds_dict.keys()):
        print(f"      {model_name.upper()}: {best_weights[i]:.4f}")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –≤–µ—Å–∞
    oof_hill = np.zeros(len(y))
    test_hill = np.zeros(len(test_preds_dict[list(test_preds_dict.keys())[0]]))
    
    for i, (model_name, preds) in enumerate(oof_preds_dict.items()):
        oof_hill += best_weights[i] * preds
        test_hill += best_weights[i] * test_preds_dict[model_name]
    
    return oof_hill, test_hill, best_weights

def run_ensemble():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("\n" + "="*70)
    print("üéØ –ê–ù–°–ê–ú–ë–õ–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("="*70)
    
    start_time = time.time()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è y
    result = load_and_preprocess()
    if len(result) == 8:
        train, test, original, num_cols, cat_cols, all_cat_cols, target_col, id_col = result
    else:
        # Fallback for old version
        train, test, num_cols, cat_cols, all_cat_cols, target_col, id_col = result
    y = train[target_col]
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (–±–µ–∑ NN, —Å multi-seed CatBoost –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
    use_multi_seed = getattr(config, 'USE_MULTI_SEED_CAT', True)
    oof_preds_dict, test_preds_dict = load_predictions(include_nn=False, include_multi_seed_cat=use_multi_seed)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    print("\n" + "="*70)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¢–î–ï–õ–¨–ù–´–• –ú–û–î–ï–õ–ï–ô")
    print("="*70)
    
    model_scores = {}
    for model_name, oof_preds in oof_preds_dict.items():
        score = roc_auc_score(y, oof_preds)
        model_scores[model_name.upper()] = score
        print(f"   {model_name.upper():<20}: {score:.5f}")
    
    # Filter out weak models (optional - can be enabled via config)
    if hasattr(config, 'ENSEMBLE_MIN_SCORE_THRESHOLD'):
        min_score_threshold = config.ENSEMBLE_MIN_SCORE_THRESHOLD
    else:
        min_score_threshold = 0.95  # Default threshold
    
    filtered_models = {k: v for k, v in oof_preds_dict.items() 
                      if model_scores[k.upper()] >= min_score_threshold}
    
    if len(filtered_models) < len(oof_preds_dict):
        removed = set(oof_preds_dict.keys()) - set(filtered_models.keys())
        print(f"\n‚ö†Ô∏è  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã —Å–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏ (CV < {min_score_threshold}): {', '.join(removed)}")
        print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è {len(filtered_models)} –∏–∑ {len(oof_preds_dict)} –º–æ–¥–µ–ª–µ–π")
        oof_preds_dict = filtered_models
        test_preds_dict = {k: test_preds_dict[k] for k in filtered_models.keys()}

    # Keep only top-K models by score (optional)
    top_k = getattr(config, "ENSEMBLE_TOP_K", None)
    if top_k is not None:
        try:
            top_k = int(top_k)
        except Exception:
            top_k = None
    if top_k is not None and top_k > 0 and len(oof_preds_dict) > top_k:
        # Sort by individual model CV (desc), tie-break by name for determinism
        sorted_models = sorted(
            oof_preds_dict.keys(),
            key=lambda m: (model_scores[m.upper()], m),
            reverse=True
        )
        keep = sorted_models[:top_k]
        removed = [m for m in oof_preds_dict.keys() if m not in keep]
        print(f"\nüìå –í—ã–±—Ä–∞–Ω—ã TOP-{top_k} –º–æ–¥–µ–ª–∏ –ø–æ CV: {', '.join([m.upper() for m in keep])}")
        if removed:
            print(f"   –£–¥–∞–ª–µ–Ω—ã: {', '.join([m.upper() for m in removed])}")
        oof_preds_dict = {k: oof_preds_dict[k] for k in keep}
        test_preds_dict = {k: test_preds_dict[k] for k in keep}
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∞–Ω—Å–∞–º–±–ª—è
    ensemble_results = {}
    
    # 1. Simple Blending
    oof_blend, test_blend = simple_blending(oof_preds_dict, test_preds_dict)
    score_blend = roc_auc_score(y, oof_blend)
    ensemble_results['Simple Blending'] = {'oof': oof_blend, 'test': test_blend, 'score': score_blend}
    print(f"   CV Score: {score_blend:.5f}")
    
    # 2. Weighted Average
    oof_weighted, test_weighted, weights = weighted_average(oof_preds_dict, test_preds_dict, y)
    score_weighted = roc_auc_score(y, oof_weighted)
    ensemble_results['Weighted Average'] = {'oof': oof_weighted, 'test': test_weighted, 'score': score_weighted, 'weights': weights}
    print(f"   CV Score: {score_weighted:.5f}")
    
    # 3. Stacking (LightGBM)
    oof_stack, test_stack = stacking(oof_preds_dict, test_preds_dict, y, meta_model_type='lgbm')
    score_stack = roc_auc_score(y, oof_stack)
    ensemble_results['Stacking (LightGBM)'] = {'oof': oof_stack, 'test': test_stack, 'score': score_stack}
    print(f"   CV Score: {score_stack:.5f}")
    
    # 3b. Stacking (Logistic Regression) - for comparison
    oof_stack_lr, test_stack_lr = stacking(oof_preds_dict, test_preds_dict, y, meta_model_type='lr')
    score_stack_lr = roc_auc_score(y, oof_stack_lr)
    ensemble_results['Stacking (LR)'] = {'oof': oof_stack_lr, 'test': test_stack_lr, 'score': score_stack_lr}
    print(f"   CV Score: {score_stack_lr:.5f}")
    
    # 4. Hill Climbing
    oof_hill, test_hill, hill_weights = hill_climbing_blend(oof_preds_dict, test_preds_dict, y)
    score_hill = roc_auc_score(y, oof_hill)
    ensemble_results['Hill Climbing'] = {'oof': oof_hill, 'test': test_hill, 'score': score_hill, 'weights': hill_weights}
    print(f"   CV Score: {score_hill:.5f}")
    
    # –ò—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print("\n" + "="*70)
    print("üèÜ –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("="*70)
    
    all_results = {**model_scores, **{k: v['score'] for k, v in ensemble_results.items()}}
    
    print(f"\n{'–ú–æ–¥–µ–ª—å/–ú–µ—Ç–æ–¥':<25} {'CV Score':<12} {'–£–ª—É—á—à–µ–Ω–∏–µ':<12}")
    print("-" * 50)
    
    best_single = max(model_scores.values())
    for name, score in sorted(all_results.items(), key=lambda x: x[1], reverse=True):
        improvement = score - best_single
        status = "‚úÖ" if improvement > 0.0001 else "‚ûñ" if improvement > -0.0001 else ""
        print(f"{name:<25} {score:.5f}      {improvement:+.5f} {status}")
    
    best_method = max(all_results.items(), key=lambda x: x[1])
    print(f"\nüèÜ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method[0]} (CV Score: {best_method[1]:.5f})")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ö–ê–ñ–î–û–ì–û –∞–Ω—Å–∞–º–±–ª—å-–º–µ—Ç–æ–¥–∞ (–≤ —Ç.—á. —Å—Ç–µ–∫–∏–Ω–≥), –ø–ª—é—Å –ª—É—á—à–∏–π –∫–∞–∫ submission_ensemble.csv
    def _slug(name: str) -> str:
        return (
            name.lower()
            .replace(" ", "_")
            .replace("(", "")
            .replace(")", "")
            .replace("__", "_")
        )
    
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ submission-—Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤—Å–µ—Ö –∞–Ω—Å–∞–º–±–ª–µ–π...")
    for method_name, payload in ensemble_results.items():
        sub = pd.DataFrame({'id': test[id_col], 'loan_status': payload['test']})
        out_path = f"submission_{_slug(method_name)}.csv"
        sub.to_csv(out_path, index=False)
        print(f"   ‚úÖ {method_name}: {out_path}")
    
    # –û—Ç–¥–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π –∞–Ω—Å–∞–º–±–ª—å –ø–æ CV –≤ submission_ensemble.csv (–∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ)
    best_ensemble = max(ensemble_results.items(), key=lambda x: x[1]['score'])
    best_test_preds = best_ensemble[1]['test']
    submission = pd.DataFrame({'id': test[id_col], 'loan_status': best_test_preds})
    submission.to_csv('submission_ensemble.csv', index=False)
    print(f"\nüèÜ –õ—É—á—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: submission_ensemble.csv")
    print(f"   –ú–µ—Ç–æ–¥: {best_ensemble[0]}")
    
    elapsed_time = time.time() - start_time
    print(f"\n‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed_time:.1f}s")
    
    return ensemble_results, model_scores

if __name__ == "__main__":
    run_ensemble()

