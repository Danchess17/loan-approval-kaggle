"""
Exploratory Data Analysis (EDA) –¥–ª—è Loan Approval Prediction
–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞, –≤—ã–±—Ä–æ—Å–æ–≤, –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ feature engineering
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

def load_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    print("="*80)
    print("üìä –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
    print("="*80)
    
    train = pd.read_csv('train.csv')
    test = pd.read_csv('test.csv')
    
    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")
    print(f"\nTrain columns: {list(train.columns)}")
    print(f"\nTrain info:")
    print(train.info())
    
    return train, test

def basic_statistics(train, test):
    """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É"""
    print("\n" + "="*80)
    print("üìà –ë–ê–ó–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print("="*80)
    
    # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
    print("\nüéØ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (loan_status):")
    print(train['loan_status'].value_counts())
    print(f"–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞: {train['loan_status'].mean()*100:.2f}%")
    
    # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    print("\n‚ùì –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ train:")
    missing_train = train.isnull().sum()
    missing_test = test.isnull().sum()
    missing_df = pd.DataFrame({
        'Train': missing_train,
        'Test': missing_test,
        'Train_%': (missing_train / len(train) * 100).round(2),
        'Test_%': (missing_test / len(test) * 100).round(2)
    })
    print(missing_df[missing_df['Train'] > 0])
    
    # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    num_cols = [
        'person_age', 'person_income', 'person_emp_length', 
        'loan_amnt', 'loan_int_rate', 'loan_percent_income', 
        'cb_person_cred_hist_length'
    ]
    
    print("\nüìä –û–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    print(train[num_cols].describe())
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    cat_cols = [
        'person_home_ownership', 'loan_intent', 'loan_grade', 
        'cb_person_default_on_file'
    ]
    
    print("\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    for col in cat_cols:
        print(f"\n{col}:")
        print(train[col].value_counts())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {train[col].nunique()}")
    
    return num_cols, cat_cols

def analyze_outliers(train, num_cols):
    """–ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤"""
    print("\n" + "="*80)
    print("üîç –ê–ù–ê–õ–ò–ó –í–´–ë–†–û–°–û–í")
    print("="*80)
    
    outlier_results = {}
    
    for col in num_cols:
        print(f"\nüìå {col}:")
        data = train[col].dropna()
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        # Z-score –º–µ—Ç–æ–¥
        z_scores = np.abs(stats.zscore(data))
        outliers_zscore = np.sum(z_scores > 3)
        
        # IQR –º–µ—Ç–æ–¥
        outliers_iqr = np.sum((data < lower_bound) | (data > upper_bound))
        
        print(f"  Min: {data.min():.2f}, Max: {data.max():.2f}")
        print(f"  Mean: {data.mean():.2f}, Median: {data.median():.2f}")
        print(f"  Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}")
        print(f"  Lower bound (IQR): {lower_bound:.2f}, Upper bound: {upper_bound:.2f}")
        print(f"  –í—ã–±—Ä–æ—Å—ã (IQR –º–µ—Ç–æ–¥): {outliers_iqr} ({outliers_iqr/len(data)*100:.2f}%)")
        print(f"  –í—ã–±—Ä–æ—Å—ã (Z-score > 3): {outliers_zscore} ({outliers_zscore/len(data)*100:.2f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        extreme_low = data[data < lower_bound]
        extreme_high = data[data > upper_bound]
        
        if len(extreme_low) > 0:
            print(f"  ‚ö†Ô∏è  –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –Ω–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: {extreme_low.min():.2f} (–≤—Å–µ–≥–æ {len(extreme_low)})")
        if len(extreme_high) > 0:
            print(f"  ‚ö†Ô∏è  –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è: {extreme_high.max():.2f} (–≤—Å–µ–≥–æ {len(extreme_high)})")
        
        outlier_results[col] = {
            'Q1': Q1,
            'Q3': Q3,
            'IQR': IQR,
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'outliers_iqr': outliers_iqr,
            'outliers_iqr_pct': outliers_iqr/len(data)*100,
            'outliers_zscore': outliers_zscore,
            'outliers_zscore_pct': outliers_zscore/len(data)*100
        }
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤—ã–±—Ä–æ—Å–æ–≤
    print("\nüìä –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –≤—ã–±—Ä–æ—Å–æ–≤...")
    fig, axes = plt.subplots(2, 4, figsize=(20, 10))
    axes = axes.flatten()
    
    for idx, col in enumerate(num_cols):
        ax = axes[idx]
        data = train[col].dropna()
        
        # Box plot
        bp = ax.boxplot(data, vert=True, patch_artist=True)
        bp['boxes'][0].set_facecolor('lightblue')
        ax.set_title(f'{col}\n–í—ã–±—Ä–æ—Å–æ–≤ (IQR): {outlier_results[col]["outliers_iqr"]} ({outlier_results[col]["outliers_iqr_pct"]:.1f}%)')
        ax.set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('outliers_analysis.png', dpi=150, bbox_inches='tight')
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: outliers_analysis.png")
    
    return outlier_results

def feature_importance_analysis(train, num_cols, cat_cols):
    """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
    print("\n" + "="*80)
    print("‚≠ê –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í")
    print("="*80)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X = train.drop(columns=['id', 'loan_status'])
    y = train['loan_status']
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    X_encoded = X.copy()
    le_dict = {}
    for col in cat_cols:
        le = LabelEncoder()
        X_encoded[col] = le.fit_transform(X[col].astype(str).fillna('Missing'))
        le_dict[col] = le
    
    # –û–±—É—á–µ–Ω–∏–µ Random Forest –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏
    print("\nüå≤ –û–±—É—á–µ–Ω–∏–µ Random Forest –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_encoded, y)
    
    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = pd.DataFrame({
        'feature': X_encoded.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nüìä –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Top 15):")
    print(feature_importance.head(15).to_string(index=False))
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(12, 8))
    top_features = feature_importance.head(15)
    sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')
    plt.title('Top 15 Most Important Features (Random Forest)', fontsize=14, fontweight='bold')
    plt.xlabel('Importance', fontsize=12)
    plt.ylabel('Feature', fontsize=12)
    plt.tight_layout()
    plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: feature_importance.png")
    
    return feature_importance

def correlation_analysis(train, num_cols):
    """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
    print("\n" + "="*80)
    print("üîó –ê–ù–ê–õ–ò–ó –ö–û–†–†–ï–õ–Ø–¶–ò–ô")
    print("="*80)
    
    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    print("\nüìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:")
    correlations = {}
    for col in num_cols:
        corr = train[col].corr(train['loan_status'])
        correlations[col] = corr
        print(f"  {col}: {corr:.4f}")
    
    # –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    corr_matrix = train[num_cols + ['loan_status']].corr()
    
    print("\nüìä –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏:")
    print(corr_matrix.round(3))
    
    # –ü–æ–∏—Å–∫ —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä
    print("\nüîç –°–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (|corr| > 0.5):")
    high_corr_pairs = []
    for i in range(len(num_cols)):
        for j in range(i+1, len(num_cols)):
            corr_val = corr_matrix.loc[num_cols[i], num_cols[j]]
            if abs(corr_val) > 0.5:
                high_corr_pairs.append((num_cols[i], num_cols[j], corr_val))
                print(f"  {num_cols[i]} <-> {num_cols[j]}: {corr_val:.3f}")
    
    if not high_corr_pairs:
        print("  –ù–µ—Ç —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä (|corr| > 0.5)")
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    plt.figure(figsize=(10, 8))
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0,
                square=True, linewidths=1, cbar_kws={"shrink": 0.8}, mask=mask)
    plt.title('Correlation Matrix (Numerical Features)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')
    print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: correlation_matrix.png")
    
    return corr_matrix, high_corr_pairs

def analyze_categorical_target_relationship(train, cat_cols):
    """–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"""
    print("\n" + "="*80)
    print("üìä –ê–ù–ê–õ–ò–ó –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í –ò –¶–ï–õ–ï–í–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô")
    print("="*80)
    
    target_rates = {}
    
    for col in cat_cols:
        print(f"\nüìå {col}:")
        grouped = train.groupby(col)['loan_status'].agg(['mean', 'count']).round(4)
        grouped.columns = ['Default_Rate', 'Count']
        grouped = grouped.sort_values('Default_Rate', ascending=False)
        print(grouped)
        
        target_rates[col] = grouped['Default_Rate'].to_dict()
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        plt.figure(figsize=(10, 6))
        grouped['Default_Rate'].plot(kind='bar', color='coral')
        plt.title(f'Default Rate by {col}', fontsize=12, fontweight='bold')
        plt.xlabel(col, fontsize=10)
        plt.ylabel('Default Rate', fontsize=10)
        plt.xticks(rotation=45, ha='right')
        plt.grid(True, alpha=0.3, axis='y')
        plt.tight_layout()
        plt.savefig(f'default_rate_{col}.png', dpi=150, bbox_inches='tight')
        print(f"  ‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: default_rate_{col}.png")
    
    return target_rates

def suggest_polynomial_features(train, num_cols, cat_cols, feature_importance, corr_matrix, target_rates):
    """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
    print("\n" + "="*80)
    print("üí° –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –ü–û–õ–ò–ù–û–ú–ò–ê–õ–¨–ù–´–ú –ü–†–ò–ó–ù–ê–ö–ê–ú")
    print("="*80)
    
    suggestions = []
    
    # 1. –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n1Ô∏è‚É£  –ù–∞ –æ—Å–Ω–æ–≤–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    top_features = feature_importance.head(10)['feature'].tolist()
    top_numeric = [f for f in top_features if f in num_cols]
    
    if len(top_numeric) >= 2:
        print(f"   –¢–æ–ø —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {top_numeric[:5]}")
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –º–µ–∂–¥—É —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        for i in range(min(3, len(top_numeric))):
            for j in range(i+1, min(4, len(top_numeric))):
                f1, f2 = top_numeric[i], top_numeric[j]
                suggestions.append({
                    'type': 'interaction',
                    'features': [f1, f2],
                    'operation': 'multiply',
                    'reason': f'–û–±–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ –≤ —Ç–æ–ø-{len(top_numeric)} –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏',
                    'priority': 'high'
                })
                print(f"   ‚úÖ {f1} * {f2} (–æ–±–∞ –≤ —Ç–æ–ø-{len(top_numeric)} –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏)")
    
    # 2. –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    print("\n2Ô∏è‚É£  –ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π:")
    # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å —É–º–µ—Ä–µ–Ω–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (0.3-0.7) - –æ–Ω–∏ –º–æ–≥—É—Ç –¥–∞—Ç—å —Ö–æ—Ä–æ—à–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
    moderate_corr_pairs = []
    for i in range(len(num_cols)):
        for j in range(i+1, len(num_cols)):
            corr_val = corr_matrix.loc[num_cols[i], num_cols[j]]
            if 0.3 <= abs(corr_val) <= 0.7:
                moderate_corr_pairs.append((num_cols[i], num_cols[j], corr_val))
    
    if moderate_corr_pairs:
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(moderate_corr_pairs)} –ø–∞—Ä —Å —É–º–µ—Ä–µ–Ω–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π (0.3-0.7):")
        for f1, f2, corr in moderate_corr_pairs[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-5
            suggestions.append({
                'type': 'interaction',
                'features': [f1, f2],
                'operation': 'multiply',
                'reason': f'–£–º–µ—Ä–µ–Ω–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è ({corr:.3f}) - –º–æ–∂–µ—Ç –≤—ã—è–≤–∏—Ç—å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏',
                'priority': 'medium'
            })
            print(f"   ‚úÖ {f1} * {f2} (–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {corr:.3f})")
    
    # 3. –ê–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏
    print("\n3Ô∏è‚É£  –ù–∞ –æ—Å–Ω–æ–≤–µ –±–∏–∑–Ω–µ—Å-–ª–æ–≥–∏–∫–∏:")
    
    # –û—Ç–Ω–æ—à–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç —Å–º—ã—Å–ª –¥–ª—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞
    business_logic_pairs = [
        (['loan_amnt', 'person_income'], 'divide', 
         '–û—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—É–º–º—ã –∑–∞–π–º–∞ –∫ –¥–æ—Ö–æ–¥—É (—É–∂–µ –µ—Å—Ç—å loan_percent_income, –Ω–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å)',
         'high'),
        (['loan_amnt', 'person_emp_length'], 'multiply',
         '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ —Å—É–º–º—ã –∑–∞–π–º–∞ –∏ —Å—Ç–∞–∂–∞ —Ä–∞–±–æ—Ç—ã',
         'medium'),
        (['person_income', 'person_emp_length'], 'multiply',
         '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –¥–æ—Ö–æ–¥–∞ –∏ —Å—Ç–∞–∂–∞ —Ä–∞–±–æ—Ç—ã (–æ–ø—ã—Ç–Ω—ã–π —Ä–∞–±–æ—Ç–Ω–∏–∫ —Å –≤—ã—Å–æ–∫–∏–º –¥–æ—Ö–æ–¥–æ–º)',
         'medium'),
        (['loan_int_rate', 'loan_amnt'], 'multiply',
         '–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–π —Å—Ç–∞–≤–∫–∏ –∏ —Å—É–º–º—ã –∑–∞–π–º–∞',
         'medium'),
        (['person_age', 'person_emp_length'], 'divide',
         '–û—Ç–Ω–æ—à–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∫ —Å—Ç–∞–∂—É (–º–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∫–∞—Ä—å–µ—Ä—ã)',
         'low'),
        (['cb_person_cred_hist_length', 'person_age'], 'divide',
         '–û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –∫ –≤–æ–∑—Ä–∞—Å—Ç—É (–ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–Ω–Ω–µ–µ –Ω–∞—á–∞–ª–æ –∫—Ä–µ–¥–∏—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏)',
         'medium'),
    ]
    
    for features, operation, reason, priority in business_logic_pairs:
        if all(f in num_cols for f in features):
            suggestions.append({
                'type': 'interaction',
                'features': features,
                'operation': operation,
                'reason': reason,
                'priority': priority
            })
            op_symbol = '*' if operation == 'multiply' else '/'
            print(f"   ‚úÖ {features[0]} {op_symbol} {features[1]} ({reason})")
    
    # 4. –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n4Ô∏è‚É£  –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Å—Ç–µ–ø–µ–Ω–∏):")
    top_3_numeric = top_numeric[:3] if len(top_numeric) >= 3 else top_numeric
    for feat in top_3_numeric:
        suggestions.append({
            'type': 'polynomial',
            'features': [feat],
            'operation': 'square',
            'reason': f'–ö–≤–∞–¥—Ä–∞—Ç –≤–∞–∂–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ (—Ç–æ–ø-3 –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏)',
            'priority': 'medium'
        })
        print(f"   ‚úÖ {feat}^2 (—Ç–æ–ø-3 –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏)")
    
    # 5. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
    print("\n5Ô∏è‚É£  –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º target rates –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    high_variance_cats = []
    for col, rates in target_rates.items():
        if len(rates) > 1:
            variance = np.var(list(rates.values()))
            if variance > 0.01:  # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è –≤ default rate
                high_variance_cats.append(col)
    
    if len(high_variance_cats) >= 2:
        for i in range(min(2, len(high_variance_cats))):
            for j in range(i+1, min(3, len(high_variance_cats))):
                f1, f2 = high_variance_cats[i], high_variance_cats[j]
                suggestions.append({
                    'type': 'categorical_interaction',
                    'features': [f1, f2],
                    'operation': 'concat',
                    'reason': f'–û–±–∞ –ø—Ä–∏–∑–Ω–∞–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é –≤–∞—Ä–∏–∞—Ü–∏—é –≤ default rate',
                    'priority': 'medium'
                })
                print(f"   ‚úÖ {f1} + {f2} (–∫–æ–º–±–∏–Ω–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π)")
    
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
    priority_order = {'high': 3, 'medium': 2, 'low': 1}
    suggestions.sort(key=lambda x: priority_order.get(x['priority'], 0), reverse=True)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    print("\n" + "="*80)
    print("üìã –ò–¢–û–ì–û–í–´–ï –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É):")
    print("="*80)
    
    high_priority = [s for s in suggestions if s['priority'] == 'high']
    medium_priority = [s for s in suggestions if s['priority'] == 'medium']
    low_priority = [s for s in suggestions if s['priority'] == 'low']
    
    print(f"\nüî¥ –í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢ ({len(high_priority)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π):")
    for idx, s in enumerate(high_priority, 1):
        op_symbol = '*' if s['operation'] == 'multiply' else '/' if s['operation'] == 'divide' else '^2'
        features_str = f" {op_symbol} ".join(s['features']) if len(s['features']) > 1 else f"{s['features'][0]}^2"
        print(f"   {idx}. {features_str}")
        print(f"      –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {s['reason']}")
    
    print(f"\nüü° –°–†–ï–î–ù–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢ ({len(medium_priority)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π):")
    for idx, s in enumerate(medium_priority, 1):
        op_symbol = '*' if s['operation'] == 'multiply' else '/' if s['operation'] == 'divide' else '^2'
        features_str = f" {op_symbol} ".join(s['features']) if len(s['features']) > 1 else f"{s['features'][0]}^2"
        print(f"   {idx}. {features_str}")
        print(f"      –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {s['reason']}")
    
    if low_priority:
        print(f"\nüü¢ –ù–ò–ó–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢ ({len(low_priority)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π):")
        for idx, s in enumerate(low_priority, 1):
            op_symbol = '*' if s['operation'] == 'multiply' else '/' if s['operation'] == 'divide' else '^2'
            features_str = f" {op_symbol} ".join(s['features']) if len(s['features']) > 1 else f"{s['features'][0]}^2"
            print(f"   {idx}. {features_str}")
            print(f"      –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {s['reason']}")
    
    return suggestions

def create_summary_report(train, test, outlier_results, feature_importance, corr_matrix, 
                         high_corr_pairs, target_rates, suggestions):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    print("\n" + "="*80)
    print("üìÑ –°–û–ó–î–ê–ù–ò–ï –ò–¢–û–ì–û–í–û–ì–û –û–¢–ß–ï–¢–ê")
    print("="*80)
    
    report = []
    report.append("="*80)
    report.append("–ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ EDA - LOAN APPROVAL PREDICTION")
    report.append("="*80)
    report.append("")
    
    # 1. –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    report.append("1. –û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –î–ê–¢–ê–°–ï–¢–ï")
    report.append("-" * 80)
    report.append(f"–†–∞–∑–º–µ—Ä train: {train.shape}")
    report.append(f"–†–∞–∑–º–µ—Ä test: {test.shape}")
    report.append(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: loan_status")
    report.append(f"–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞: {train['loan_status'].mean()*100:.2f}%")
    report.append("")
    
    # 2. –í—ã–±—Ä–æ—Å—ã
    report.append("2. –ê–ù–ê–õ–ò–ó –í–´–ë–†–û–°–û–í")
    report.append("-" * 80)
    for col, results in outlier_results.items():
        report.append(f"{col}:")
        report.append(f"  - –í—ã–±—Ä–æ—Å—ã (IQR): {results['outliers_iqr']} ({results['outliers_iqr_pct']:.2f}%)")
        report.append(f"  - –í—ã–±—Ä–æ—Å—ã (Z-score): {results['outliers_zscore']} ({results['outliers_zscore_pct']:.2f}%)")
    report.append("")
    
    # 3. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    report.append("3. –¢–û–ü-10 –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í")
    report.append("-" * 80)
    for idx, row in feature_importance.head(10).iterrows():
        report.append(f"{idx+1}. {row['feature']}: {row['importance']:.4f}")
    report.append("")
    
    # 4. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    report.append("4. –°–ò–õ–¨–ù–û –ö–û–†–†–ï–õ–ò–†–û–í–ê–ù–ù–´–ï –ü–ê–†–´ (|corr| > 0.5)")
    report.append("-" * 80)
    if high_corr_pairs:
        for f1, f2, corr in high_corr_pairs:
            report.append(f"{f1} <-> {f2}: {corr:.3f}")
    else:
        report.append("–ù–µ—Ç —Å–∏–ª—å–Ω–æ –∫–æ—Ä—Ä–µ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–∞—Ä")
    report.append("")
    
    # 5. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    report.append("5. –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø –ü–û –ü–û–õ–ò–ù–û–ú–ò–ê–õ–¨–ù–´–ú –ü–†–ò–ó–ù–ê–ö–ê–ú")
    report.append("-" * 80)
    high_priority = [s for s in suggestions if s['priority'] == 'high']
    report.append(f"\n–í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç ({len(high_priority)}):")
    for s in high_priority:
        op_symbol = '*' if s['operation'] == 'multiply' else '/' if s['operation'] == 'divide' else '^2'
        features_str = f" {op_symbol} ".join(s['features']) if len(s['features']) > 1 else f"{s['features'][0]}^2"
        report.append(f"  - {features_str}: {s['reason']}")
    
    report.append("")
    report.append("="*80)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    report_text = "\n".join(report)
    with open('eda_report.txt', 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print("‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: eda_report.txt")
    print("\n" + report_text)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ EDA"""
    print("\n" + "üöÄ"*40)
    print("–ù–ê–ß–ê–õ–û EDA –ê–ù–ê–õ–ò–ó–ê")
    print("üöÄ"*40 + "\n")
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train, test = load_data()
    
    # 2. –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    num_cols, cat_cols = basic_statistics(train, test)
    
    # 3. –ê–Ω–∞–ª–∏–∑ –≤—ã–±—Ä–æ—Å–æ–≤
    outlier_results = analyze_outliers(train, num_cols)
    
    # 4. –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_importance = feature_importance_analysis(train, num_cols, cat_cols)
    
    # 5. –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    corr_matrix, high_corr_pairs = correlation_analysis(train, num_cols)
    
    # 6. –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    target_rates = analyze_categorical_target_relationship(train, cat_cols)
    
    # 7. –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –ø–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º
    suggestions = suggest_polynomial_features(
        train, num_cols, cat_cols, feature_importance, 
        corr_matrix, target_rates
    )
    
    # 8. –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    create_summary_report(
        train, test, outlier_results, feature_importance,
        corr_matrix, high_corr_pairs, target_rates, suggestions
    )
    
    print("\n" + "‚úÖ"*40)
    print("EDA –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
    print("‚úÖ"*40)
    print("\n–°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print("  - outliers_analysis.png")
    print("  - feature_importance.png")
    print("  - correlation_matrix.png")
    print("  - default_rate_*.png (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞)")
    print("  - eda_report.txt")
    print("\n")

if __name__ == "__main__":
    main()

